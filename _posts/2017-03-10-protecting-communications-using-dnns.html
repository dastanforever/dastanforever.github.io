---
layout: post
date:   2017-03-04 11:50:32 +0530
title: Modelling Adversarial Communication using DNNs
author: pranav
permalink: /machine_learning/crypto_dnn/
categories: machine_learning
share_content: Modelling Adversarial Communication using DNN, network that self learns encryption process.
image_sharing: https://camo.githubusercontent.com/5610aa0fe10ed3ba44fbce582d199d2f8e23de4d/687474703a2f2f7072736861726d612e6d652f6173736574732f696d616765732f616476657273617269616c5f63727970746f2f6576655f6c6f73732e706e67
support_data: {
      links: { "http://arxiv.org/pdf/1610.06918.pdf": "Google Brain Paper", 
                "https://github.com/phraniiac/adversarial_cryptography_using_tensorflow": "Github Repo"},
      tags: {"cryptography,", "deep neural networks", "tensorflow"},
      categories: {"machine_learning"},
      }
---

This midsems, my Cryptosystem teacher advised to either take exams seriously or to make a project. Well, the exams never bothered
me anyway :p (and because of that I don't score good in them), but I really thought of making something for project that would 
also help me improve my understanding of deep neural nets. So after researching, I found a good paper published by Google Brain 
Team, the link is in description, which had some proofs that a network can learn to protect information from adversaries. So I 
decided to implement model they presented. This would make a good networking project and I would also produce visualizations from
TensorBoard, that I needed for <a href="{{site.baseurl}}/machine_learning/tensorboard/">this blog post</a>.

<br>
<br>
<type2>TLDR (for research paper)</type2>
<br>
The famous old example for Alice, Bob, and Eve is used in the discussion. Check the model - 

<figure>
    <img src="{{ site.baseurl }}{% link /assets/images/adversarial_crypto/crypto_model_1.png %}" />
    <figcaption> Alice, Bob, and Eve, with a symmetric cryptosystem.</figcaption>
</figure>

There are convnets, and fully-connected layers implemented for all three, but the basic difference is of the key. It can be viewed
as a generative adversarial neural network (according to <a href="https://en.wikipedia.org/wiki/Generative_adversarial_networks">wikipedia</a>,
these are the ones, that train while competing), having two optimizers, which oppose each other, but with single network. As all other parameters
are the same for the optimizers, the difference lies in the network itself, which is the key is not provided with the input at the Eve node. The 
optimizer which has the key outperforms the one without, and hence results in large error bits on the Eve node.

<br>
<br>
<type2>Results</type2>
<br>
Well, I didn't go onto the scale the paper suggests (limitation of resources), but the visualizations here are pretty promising that
the network is learning. Below are the error bit rates. I believe if trained long enough, the model would approach 0 bits for bob, and
almost 8 bits for eve as said in the paper, although here too, the relation can be seen distinctively, with 2 and 11 bits respectively.

The visualizations are created using TensorBoard, as told in this <a href="{{site.baseurl}}/machine_learning/tensorboard">post</a>.
<br>
<br>
<figure>
    <img class="detail_img" src="{{ site.baseurl }}{% link /assets/images/adversarial_crypto/bits_bob.png%}" />
    <figcaption>Wrong bits that Bob predicted. Almost 2 bits.</figcaption>
</figure>

<figure>
    <img class="detail_img" src="{{ site.baseurl }}{% link /assets/images/adversarial_crypto/bits_eve.png%}" />
    <figcaption>Wrong bits that Eve predicted. Almost 11 bits.</figcaption>
</figure>